{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"ad6bcd27861e4b8fa29b9eef48ce2e0a","deepnote_cell_type":"markdown"},"source":"#### DivideFromHere","block_group":"2fcb52ac7be74779be488b20de452d3f"},{"cell_type":"markdown","metadata":{"cell_id":"4f3fad7aa0654ea6a96f772e8ca48a56","deepnote_cell_type":"markdown"},"source":"# Experiment 4: DQN based Abel and Its derandomization and KAN integration","block_group":"112aa43bb72e4e1db8aca6cb65bfcfa4"},{"cell_type":"markdown","metadata":{"cell_id":"849c265253c348938be228f298850aa3","deepnote_cell_type":"markdown"},"source":"## Step 1: Overall Design\n### Objective:\nTo implement and analyze the equivalence between probabilistic and deterministic versions of a Deep Q-Network (DQN) based chess AI named Abel using Kolmogorov-Arnold Networks (KAN).\n\n### Steps:\n1. Data Preparation:\n\n- Train the Simple NN-based Abel with both probabilistic and deterministic versions.\n- Collect performance metrics such as material count, mobility count, piece-square score, and center control count.\n2. Training and Evaluation:\n\n- Define a custom KAN model architecture using PyTorch.\n- Train the KAN model on the collected data.\n- Evaluate the model's performance and track the equivalence score during training.\n3. Visualization:\n\n- Visualize the dataset.\n- Plot the equivalence curve to show the relationship between the deterministic and probabilistic versions.\n- Extract and visualize the symbolic formula from the trained KAN model.\n- Plot the model's structure and equivalence data points.\n- Visualize the weights and biases of the trained KAN model.","block_group":"113a4f9030e74e8d97f1d2eaf0197308"},{"cell_type":"markdown","metadata":{"cell_id":"3c2e6fb91f634036b0e76ea4f4b40d6c","deepnote_cell_type":"markdown"},"source":"","block_group":"aae0fd7f16724b338abf9066cacee41d"},{"cell_type":"markdown","metadata":{"cell_id":"5d35658d6c6549fdbdc9d53200f6ac1b","deepnote_cell_type":"markdown"},"source":"## Step 2: Define the DQN for Abel\n### 2.1: Define the Neural Network for DQN","block_group":"3b81ef2778db4e0994695e00e84f79f8"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"1fc38098b15a41e7abaecab2b1de2a4c","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport chess\nfrom collections import deque\nimport random\nfrom IPython.display import display, clear_output, SVG\nimport time\n\n# Deep Q-Network model for chess evaluation\nclass AbelDQN:\n    def __init__(self, seed):\n        self.model = self.build_model(seed)\n    \n    def build_model(self, seed):\n        np.random.seed(seed)\n        tf.random.set_seed(seed)\n        model = Sequential([\n            Dense(64, activation='relu', input_dim=64),\n            Dense(64, activation='relu'),\n            Dense(1, activation='linear')\n        ])\n        model.compile(optimizer='adam', loss='mse')\n        return model\n    \n    def evaluate_board(self, board):\n        board_state = self.board_to_input(board)\n        return self.model.predict(board_state, verbose=0)[0][0]\n    \n    def board_to_input(self, board):\n        board_state = np.zeros(64)\n        for i, piece in board.piece_map().items():\n            board_state[i] = piece.piece_type if piece.color == chess.WHITE else -piece.piece_type\n        return np.array([board_state])\n","block_group":"ee7cda7215164be98a3b40f653fae7e7","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"bf10eb5a75b9431087641248e59bc52f","deepnote_cell_type":"markdown"},"source":"### 2.2: Derandomization functions","block_group":"552c2bf926984d3b9c4ce4b125007f02"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"9901665d225a461bb61623158eb4842e","deepnote_cell_type":"code"},"source":"def set_random_seed(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\ndef softmax_policy(q_values, legal_moves, temperature):\n    q_values = np.array(q_values)  # Convert list to numpy array\n    exp_values = np.exp(q_values / temperature)\n    probabilities = exp_values / np.sum(exp_values)\n    return legal_moves[np.argmax(probabilities)]\n","block_group":"1e647a06c6d84dc5a498c709607cfbff","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"f04f5f4070184443a14b443103e5611e","deepnote_cell_type":"markdown"},"source":"## Step 3: Training Functions\n### 3.1: Training the Original DQN","block_group":"e478d46319a34bd59d3d26175c307b53"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"4d9a1fbfb0144852bd66a16ac01f5996","deepnote_cell_type":"code"},"source":"def train_dqn(env, num_episodes, seed, temperature):\n    set_random_seed(seed)\n    dqn_abel = AbelDQN(seed)\n    target_network = AbelDQN(seed)\n    replay_buffer = deque(maxlen=2000)\n    gamma = 0.99\n    batch_size = 32\n    target_update_freq = 10\n\n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            q_values = [dqn_abel.evaluate_board(env.simulate_move(state, move)) for move in env.legal_moves(state)]\n            action = softmax_policy(q_values, env.legal_moves(state), temperature)\n            next_state, reward, done = env.step(state, action)\n            replay_buffer.append((state, action, reward, next_state, int(done)))\n            state = next_state\n            \n            if len(replay_buffer) > batch_size:\n                batch = random.sample(replay_buffer, batch_size)\n                update_dqn(dqn_abel, target_network, batch, gamma)\n        \n        if episode % target_update_freq == 0:\n            target_network.model.set_weights(dqn_abel.model.get_weights())\n    \n    return dqn_abel\n\ndef update_dqn(dqn_abel, target_network, batch, gamma):\n    states, actions, rewards, next_states, dones = zip(*batch)\n    target_q_values = [target_network.evaluate_board(state) for state in next_states]\n    targets = rewards + (1 - np.array(dones)) * gamma * np.array(target_q_values)\n    states = np.array([dqn_abel.board_to_input(state)[0] for state in states])\n    print(f\"States shape: {states.shape}, Targets shape: {targets.shape}\")\n    dqn_abel.model.train_on_batch(states, targets)\n","block_group":"2dfa8d96e0394b38850ef68de8057774","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"eb95deb6abce4bba986b64ccf540450f","deepnote_cell_type":"markdown"},"source":"### 3.2: Training the Derandomized DQN","block_group":"cd2fe386597f463c9ab3b29a922549bf"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"7f8c029d41df47e3ab8a24041ad0826a","deepnote_cell_type":"code"},"source":"def train_abel_dqn_derandomized(env, num_episodes, seed, temperature):\n    set_random_seed(seed)\n    dqn_abel = AbelDQN(seed)\n    target_network = AbelDQN(seed)\n    replay_buffer = deque(maxlen=2000)\n    gamma = 0.99\n    batch_size = 32\n    target_update_freq = 10\n\n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            q_values = [dqn_abel.evaluate_board(env.simulate_move(state, move)) for move in env.legal_moves(state)]\n            action = softmax_policy(q_values, env.legal_moves(state), temperature)\n            next_state, reward, done = env.step(state, action)\n            replay_buffer.append((state, action, reward, next_state, int(done)))\n            state = next_state\n            \n            if len(replay_buffer) > batch_size:\n                batch = random.sample(replay_buffer, batch_size)\n                update_dqn(dqn_abel, target_network, batch, gamma)\n        \n        if episode % target_update_freq == 0:\n            target_network.model.set_weights(dqn_abel.model.get_weights())\n    \n    return dqn_abel\n","block_group":"8650c64bdb704ea89c70dd880334e236","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"b068b6a084d04fa78a1b18ea5dfd0b02","deepnote_cell_type":"markdown"},"source":"## Step 4: Simulation Functions\n### 4.1: Environment and Simulation Setup","block_group":"e585834d2558483792db4271841cf0e4"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"37497bc90c3c4f67bd198e414fda23c9","deepnote_cell_type":"code"},"source":"# Environment simulation\nclass ChessEnv:\n    def reset(self):\n        self.board = chess.Board()\n        return self.board\n\n    def step(self, state, action):\n        self.board.push(action)\n        reward = self.evaluate_board(self.board)\n        done = self.board.is_game_over()\n        next_state = self.board\n        return next_state, reward, done\n\n    def legal_moves(self, state):\n        return list(state.legal_moves)\n\n    def simulate_move(self, state, move):\n        board_copy = state.copy()\n        board_copy.push(move)\n        return board_copy\n\n    def evaluate_board(self, board):\n        return sum(1 if piece.color == chess.WHITE else -1 for piece in board.piece_map().values())\n\n# Function to calculate additional metrics\ndef calculate_metrics(board):\n    material_count = sum(1 if piece.color == chess.WHITE else -1 for piece in board.piece_map().values())\n    mobility_count = len(list(board.legal_moves))\n    piece_square_score = sum(1 if piece.color == chess.WHITE else -1 for piece in board.piece_map().values())\n    center_control_count = sum(1 if square in [chess.D4, chess.E4, chess.D5, chess.E5] else 0 for square, piece in board.piece_map().items())\n    return material_count, mobility_count, piece_square_score, center_control_count\n\ndef calculate_additional_metrics(board, move_scores, current_depth, is_exploratory):\n    evaluation_score = sum(move_scores) / len(move_scores) if move_scores else 0\n    branching_factor = len(list(board.legal_moves))\n    depth_of_search = current_depth\n    move_diversity = np.var(move_scores) if move_scores else 0\n    exploration_vs_exploitation = 1 if is_exploratory else 0\n    return evaluation_score, branching_factor, depth_of_search, move_diversity, exploration_vs_exploitation\n","block_group":"099a4820622644dea7ce8f1175330a0d","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"70a63aa2e43b4f018e016321073bdf83","deepnote_cell_type":"markdown"},"source":"### 4.2: Simulate Games using Probabilistic DQN","block_group":"c54467c2792e4f06ab365d05aeed30f6"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"1e8f0510f51d472da06d2b0dd6225bfd","deepnote_cell_type":"code"},"source":"# Function to play the game with probabilistic DQN\ndef play_game_dqn_probabilistic(dqn_abel, env, max_moves=55, max_runtime=600, temperature=1.0):\n    steps, times, material_counts, mobility_counts, piece_square_scores, center_control_counts, move_list = [], [], [], [], [], [], []\n    evaluation_scores, branching_factors, depths_of_search, move_diversities, exploration_vs_exploitations = [], [], [], [], []\n    step_number = 1\n    state = env.reset()\n    start_time = time.time()\n\n    while not env.board.is_game_over() and step_number <= max_moves and (time.time() - start_time) <= max_runtime:\n        move_start_time = time.time()\n        q_values = [dqn_abel.evaluate_board(env.simulate_move(state, move)) for move in env.legal_moves(state)]\n        action = softmax_policy(q_values, env.legal_moves(state), temperature)\n        next_state, reward, done = env.step(state, action)\n        move_end_time = time.time()\n\n        move_list.append(action.uci())\n        steps.append(step_number)\n        times.append(move_end_time - move_start_time)\n        material_count, mobility_count, piece_square_score, center_control_count = calculate_metrics(env.board)\n        material_counts.append(material_count)\n        mobility_counts.append(mobility_count)\n        piece_square_scores.append(piece_square_score)\n        center_control_counts.append(center_control_count)\n\n        move_scores = [dqn_abel.evaluate_board(env.simulate_move(state, move)) for move in env.legal_moves(state)]\n        evaluation_score, branching_factor, depth_of_search, move_diversity, exploration_vs_exploitation = calculate_additional_metrics(\n            env.board, move_scores, 3, False)  # Depth = 3, is_exploratory = False as placeholder\n        evaluation_scores.append(evaluation_score)\n        branching_factors.append(branching_factor)\n        depths_of_search.append(depth_of_search)\n        move_diversities.append(move_diversity)\n        exploration_vs_exploitations.append(exploration_vs_exploitation)\n\n        state = next_state\n        step_number += 1\n\n        clear_output(wait=True)\n        display(SVG(chess.svg.board(board=env.board, size=350)))\n        time.sleep(1)\n        print(f\"Move: {action}, Step: {step_number}, Time: {move_end_time - move_start_time}, Material: {reward}\")\n\n    data = {\n        'Step': steps,\n        'Time': times,\n        'Move': move_list,\n        'Material Count': material_counts,\n        'Mobility Count': mobility_counts,\n        'Piece-Square Score': piece_square_scores,\n        'Center Control Count': center_control_counts,\n        'Evaluation Score': evaluation_scores,\n        'Branching Factor': branching_factors,\n        'Depth of Search': depths_of_search,\n        'Move Diversity': move_diversities,\n        'Exploration vs Exploitation': exploration_vs_exploitations\n    }\n    df = pd.DataFrame(data)\n    print(f\"Result: {env.board.result()}\")\n    return df\n\n# Initialize the environment and the network\nenv = ChessEnv()\ndqn_abel_probabilistic = train_dqn(env, num_episodes=5, seed=42, temperature=1.0)\n\n# Simulate and run the probabilistic game\nprint(\"Running probabilistic game...\")\nprobabilistic_dqn_results = play_game_dqn_probabilistic(dqn_abel_probabilistic, env, temperature=0.5)\n","block_group":"2d9a7c391d2648ec82471da533c87994","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"994a578517124efdbecaf596f2a9713e","deepnote_cell_type":"markdown"},"source":"### 4.3: Simulate Games using Derandomized DQN","block_group":"f9515ac315fa4c7bb9638f84fee70949"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"5711fbe7b4ed44869d7d8a015a6b6298","deepnote_cell_type":"code"},"source":"# Function to play the game with derandomized DQN\ndef play_game_dqn_derandomized(dqn_abel, env, max_moves=55, max_runtime=600):\n    steps, times, material_counts, mobility_counts, piece_square_scores, center_control_counts, move_list = [], [], [], [], [], [], []\n    evaluation_scores, branching_factors, depths_of_search, move_diversities, exploration_vs_exploitations = [], [], [], [], []\n    step_number = 1\n    state = env.reset()\n    start_time = time.time()\n\n    while not env.board.is_game_over() and step_number <= max_moves and (time.time() - start_time) <= max_runtime:\n        move_start_time = time.time()\n        q_values = [dqn_abel.evaluate_board(env.simulate_move(state, move)) for move in env.legal_moves(state)]\n        action = softmax_policy(q_values, env.legal_moves(state), 1.0)  # Temperature = 1.0 for deterministic\n        next_state, reward, done = env.step(state, action)\n        move_end_time = time.time()\n\n        move_list.append(action.uci())\n        steps.append(step_number)\n        times.append(move_end_time - move_start_time)\n        material_count, mobility_count, piece_square_score, center_control_count = calculate_metrics(env.board)\n        material_counts.append(material_count)\n        mobility_counts.append(mobility_count)\n        piece_square_scores.append(piece_square_score)\n        center_control_counts.append(center_control_count)\n\n        move_scores = [dqn_abel.evaluate_board(env.simulate_move(state, move)) for move in env.legal_moves(state)]\n        evaluation_score, branching_factor, depth_of_search, move_diversity, exploration_vs_exploitation = calculate_additional_metrics(\n            env.board, move_scores, 3, False)  # Depth = 3, is_exploratory = False as placeholder\n        evaluation_scores.append(evaluation_score)\n        branching_factors.append(branching_factor)\n        depths_of_search.append(depth_of_search)\n        move_diversities.append(move_diversity)\n        exploration_vs_exploitations.append(exploration_vs_exploitation)\n\n        state = next_state\n        step_number += 1\n\n        clear_output(wait=True)\n        display(SVG(chess.svg.board(board=env.board, size=350)))\n        time.sleep(0.1)  # Reduced sleep time for faster execution\n\n    data = {\n        'Step': steps,\n        'Time': times,\n        'Move': move_list,\n        'Material Count': material_counts,\n        'Mobility Count': mobility_counts,\n        'Piece-Square Score': piece_square_scores,\n        'Center Control Count': center_control_counts,\n        'Evaluation Score': evaluation_scores,\n        'Branching Factor': branching_factors,\n        'Depth of Search': depths_of_search,\n        'Move Diversity': move_diversities,\n        'Exploration vs Exploitation': exploration_vs_exploitations\n    }\n    df = pd.DataFrame(data)\n    print(f\"Result: {env.board.result()}\")\n    return df\n\n# Initialize the derandomized network\ndqn_abel_derandomized = train_abel_dqn_derandomized(env, num_episodes=5, seed=42, temperature=1.0)\n\n# Simulate and run the derandomized game\nprint(\"Running derandomized game...\")\nderandomized_dqn_results = play_game_dqn_derandomized(dqn_abel_derandomized, env)\n","block_group":"98a15e0c8d164092979fd433c961871b","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"9dea46d93eef479884dfa3a8c24c96b8","deepnote_cell_type":"markdown"},"source":"## Step 5: Collect and Analyze Performance Metrics","block_group":"02c145436cf24a66b113d5ff496c1e40"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"2ab5b631d3354dae9e92cd93f1165be2","deepnote_cell_type":"code"},"source":"def aggregate_dqn_metrics(results):\n    numeric_columns = ['Material Count', 'Mobility Count', 'Piece-Square Score', 'Center Control Count',\n                       'Evaluation Score', 'Branching Factor', 'Depth of Search', 'Move Diversity',\n                       'Exploration vs Exploitation']\n    \n    aggregated_data = pd.concat(results, ignore_index=True)\n    mean_metrics = aggregated_data[numeric_columns].mean()\n    std_metrics = aggregated_data[numeric_columns].std()\n    \n    return mean_metrics, std_metrics, aggregated_data\n\n# Aggregate the results\nprobabilistic_dqn_mean, probabilistic_dqn_std, probabilistic_dqn_data = aggregate_dqn_metrics([probabilistic_dqn_results])\nderandomized_dqn_mean, derandomized_dqn_std, derandomized_dqn_data = aggregate_dqn_metrics([derandomized_dqn_results])\n\n# Display the aggregated metrics\nprint(\"Probabilistic DQN Mean Metrics:\\n\", probabilistic_dqn_mean)\nprint(\"Probabilistic DQN Std Metrics:\\n\", probabilistic_dqn_std)\nprint(\"\\nDerandomized DQN Mean Metrics:\\n\", derandomized_dqn_mean)\nprint(\"Derandomized DQN Std Metrics:\\n\", derandomized_dqn_std)\n\n# Display move sequences and non-numeric data\nprint(\"\\nProbabilistic Moves:\\n\", probabilistic_dqn_data['Move'])\nprint(\"\\nDerandomized Moves:\\n\", derandomized_dqn_data['Move'])\n","block_group":"96588a3f865d41a6b892891e74f84546","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"edd3ee1e4c8a444f89564f0fc336f825","deepnote_cell_type":"markdown"},"source":"## Step 6: Generate and Analyze Equivalence Curves","block_group":"58bb1f7560ab460abf7e817363d8dc2d"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"86f315455f43496d872f74f5ddab1e21","deepnote_cell_type":"code"},"source":"def plot_dqn_equivalence_curve(derandomized_mean, derandomized_std, probabilistic_mean, probabilistic_std):\n    metrics = derandomized_mean.index\n    x = range(len(metrics))\n\n    fig, ax = plt.subplots(figsize=(14, 7))\n\n    ax.errorbar(x, derandomized_mean, yerr=derandomized_std, fmt='o-', label='Derandomized', color='blue', capsize=5)\n    ax.errorbar(x, probabilistic_mean, yerr=probabilistic_std, fmt='o-', label='Probabilistic', color='green', capsize=5)\n\n    ax.fill_between(x, derandomized_mean - derandomized_std, derandomized_mean + derandomized_std, color='blue', alpha=0.2)\n    ax.fill_between(x, probabilistic_mean - probabilistic_std, probabilistic_mean + probabilistic_std, color='green', alpha=0.2)\n\n    ax.set_title('Equivalence Curve for Derandomized and Probabilistic DQN')\n    ax.set_xlabel('Metrics')\n    ax.set_ylabel('Values')\n    ax.set_xticks(x)\n    ax.set_xticklabels(metrics, rotation=45, ha='right')\n\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n# Plot equivalence curve\nplot_dqn_equivalence_curve(derandomized_dqn_mean, derandomized_dqn_std, probabilistic_dqn_mean, probabilistic_dqn_std)\n","block_group":"27b47eb9d46f4940a025bb6e2613aae1","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"5e332d00ef1d46a69307aa29a0bf0d00","deepnote_cell_type":"markdown"},"source":"## Step 7: Verification and Conclusion\n### 7.1: Simulate Multiple Games","block_group":"180780af767c4a5c8c364771d574d34a"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"965739c74024472ab6574d2b3282b46c","deepnote_cell_type":"code"},"source":"def compare_dqn_versions(derandomized_dqn, probabilistic_dqn, env, games=5, max_moves=55, max_runtime=600, temperature=0.5):\n    derandomized_results = []\n    probabilistic_results = []\n\n    for _ in range(games):\n        derandomized_data = play_game_dqn_derandomized(derandomized_dqn, env, max_moves, max_runtime)\n        derandomized_results.append(derandomized_data)\n\n        probabilistic_data = play_game_dqn_probabilistic(probabilistic_dqn, env, max_moves, max_runtime, temperature)\n        probabilistic_results.append(probabilistic_data)\n\n    return derandomized_results, probabilistic_results\n\n# Compare the derandomized and probabilistic DQN versions over multiple games\nderandomized_dqn_results_multiple, probabilistic_dqn_results_multiple = compare_dqn_versions(dqn_abel_derandomized, dqn_abel_probabilistic, env)\n","block_group":"c571c9e062a3419da59d1b257e313292","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"32388823deeb4426b9a9dbaf474c8f4d","deepnote_cell_type":"markdown"},"source":"### 7.2: Collect and Aggregate Performance Metrics","block_group":"ff9e5c3a27ec407fa8b8f1b839a78e23"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"cccaf95ea8454c5882515371ce67e5ef","deepnote_cell_type":"code"},"source":"# Aggregate the results\nderandomized_dqn_mean_multiple, derandomized_dqn_std_multiple, derandomized_dqn_data_multiple = aggregate_dqn_metrics(derandomized_dqn_results_multiple)\nprobabilistic_dqn_mean_multiple, probabilistic_dqn_std_multiple, probabilistic_dqn_data_multiple = aggregate_dqn_metrics(probabilistic_dqn_results_multiple)\n\n# Display the aggregated metrics\nprint(\"Derandomized DQN Mean Metrics (Multiple Games):\\n\", derandomized_dqn_mean_multiple)\nprint(\"Derandomized DQN Std Metrics (Multiple Games):\\n\", derandomized_dqn_std_multiple)\nprint(\"\\nProbabilistic DQN Mean Metrics (Multiple Games):\\n\", probabilistic_dqn_mean_multiple)\nprint(\"Probabilistic DQN Std Metrics (Multiple Games):\\n\", probabilistic_dqn_std_multiple)\n","block_group":"6109d06fe72a4c1eb3bc3b53fa0051ec","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"a65dddf01f8145ee8763aca6f636a110","deepnote_cell_type":"markdown"},"source":"### 7.3: Generate and Analyze Equivalence Curves","block_group":"e471d2023ac84357b79337085f1982df"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"f363d226f6f747e29d9ac3c0ec503c7d","deepnote_cell_type":"code"},"source":"# Plot equivalence curve\nplot_dqn_equivalence_curve(derandomized_dqn_mean_multiple, derandomized_dqn_std_multiple, probabilistic_dqn_mean_multiple, probabilistic_dqn_std_multiple)\n","block_group":"b390d5826a1340bd98138d19d20cab75","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"8f847f64a6ba40989cf831c4b63e2b1e","deepnote_cell_type":"markdown"},"source":"### 7.4: Run Additional Games to Gather More Data","block_group":"b7af2037f9f6457e84d5b8debeef4e2d"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"b9a651cc45e3485a862064560e943673","deepnote_cell_type":"code"},"source":"# Run additional games to gather more data\nadditional_games = 100\nderandomized_dqn_results_additional, probabilistic_dqn_results_additional = compare_dqn_versions(dqn_abel_derandomized, dqn_abel_probabilistic, env, games=additional_games, temperature=0.5)\n\n# Aggregate the additional data\nderandomized_dqn_mean_additional, derandomized_dqn_std_additional, derandomized_dqn_data_additional = aggregate_dqn_metrics(derandomized_dqn_results_additional)\nprobabilistic_dqn_mean_additional, probabilistic_dqn_std_additional, probabilistic_dqn_data_additional = aggregate_dqn_metrics(probabilistic_dqn_results_additional)\n\n# Combine the original and additional data\ncombined_derandomized_dqn_data = pd.concat([derandomized_dqn_data_multiple, derandomized_dqn_data_additional], ignore_index=True)\ncombined_probabilistic_dqn_data = pd.concat([probabilistic_dqn_data_multiple, probabilistic_dqn_data_additional], ignore_index=True)\n\n# Recalculate the means and standard deviations\ncombined_derandomized_dqn_mean, combined_derandomized_dqn_std = combined_derandomized_dqn_data.mean(), combined_derandomized_dqn_data.std()\ncombined_probabilistic_dqn_mean, combined_probabilistic_dqn_std = combined_probabilistic_dqn_data.mean(), combined_probabilistic_dqn_data.std()\n\n# Display the combined metrics\nprint(\"Combined Derandomized DQN Mean Metrics:\\n\", combined_derandomized_dqn_mean)\nprint(\"Combined Derandomized DQN Std Metrics:\\n\", combined_derandomized_dqn_std)\nprint(\"\\nCombined Probabilistic DQN Mean Metrics:\\n\", combined_probabilistic_dqn_mean)\nprint(\"Combined Probabilistic DQN Std Metrics:\\n\", combined_probabilistic_dqn_std)\n\n# Plot combined equivalence curve\nplot_dqn_equivalence_curve(combined_derandomized_dqn_mean, combined_derandomized_dqn_std, combined_probabilistic_dqn_mean, combined_probabilistic_dqn_std)\n","block_group":"ee779d686c73450d852b96801ed88a29","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"bc0f42de076f47188f18f24b5ed33503","deepnote_cell_type":"markdown"},"source":"## Step 8: Perform Statistical Tests","block_group":"a25b1c889dbe45d4875688dbf098590b"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"30f4ca1a592a4510900f72066012aa24","deepnote_cell_type":"code"},"source":"from scipy.stats import ttest_ind, f_oneway\n\ndef perform_statistical_tests(derandomized_metrics, probabilistic_metrics):\n    results = {}\n    for metric in derandomized_metrics.index:\n        t_stat, p_value_t = ttest_ind(derandomized_metrics[metric], probabilistic_metrics[metric], equal_var=False)\n        f_stat, p_value_f = f_oneway(derandomized_metrics[metric], probabilistic_metrics[metric])\n        results[metric] = {\n            't_stat': t_stat,\n            'p_value_t': p_value_t,\n            'f_stat': f_stat,\n            'p_value_f': p_value_f\n        }\n    return results\n\n# Perform statistical tests\nstatistical_results = perform_statistical_tests(combined_derandomized_dqn_mean, combined_probabilistic_dqn_mean)\n\n# Display the results\nfor metric, result in statistical_results.items():\n    print(f\"{metric}: t-statistic = {result['t_stat']}, p-value (t-test) = {result['p_value_t']}\")\n    print(f\"{metric}: f-statistic = {result['f_stat']}, p-value (F-test) = {result['p_value_f']}\\n\")\n\n# Plot statistical analysis results\ndef plot_statistical_analysis(statistical_results):\n    metrics = list(statistical_results.keys())\n    t_stats = [result['t_stat'] for result in statistical_results.values()]\n    p_values_t = [result['p_value_t'] for result in statistical_results.values()]\n\n    fig, axs = plt.subplots(2, 1, figsize=(14, 10))\n\n    # Plot t-statistics\n    axs[0].bar(metrics, t_stats, color='blue')\n    axs[0].set_xticks(metrics)\n    axs[0].set_xticklabels(metrics, rotation=45, ha='right')\n    axs[0].set_ylabel('t-statistic')\n    axs[0].set_title('t-statistic of Each Metric')\n\n    # Plot p-values (t-test)\n    axs[1].bar(metrics, p_values_t, color='green')\n    axs[1].set_xticks(metrics)\n    axs[1].set_xticklabels(metrics, rotation=45, ha='right')\n    axs[1].axhline(y=0.05, color='r', linestyle='--')\n    axs[1].set_ylabel('p-value (t-test)')\n    axs[1].set_title('p-value (t-test) of Each Metric')\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot statistical analysis results\nplot_statistical_analysis(statistical_results)\n","block_group":"0447b202a8d04fdf92001c2bd3b3dcd3","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"09290dca2b43457cb802992b4d6b67b4","deepnote_cell_type":"markdown"},"source":"## Step 9: Integrate KAN for Interpretation\n### 9.1: Define the KAN Model","block_group":"e1d3f31b54e64fd0ad93c7c113cd4a92"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"652df6c419cb4df5b8d41bec742108a7","deepnote_cell_type":"code"},"source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass KANModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(KANModel, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\ndef train_kan_model(kan_model, data_loader, criterion, optimizer, num_epochs=100):\n    for epoch in range(num_epochs):\n        for inputs, targets in data_loader:\n            inputs, targets = inputs.float(), targets.float()\n            optimizer.zero_grad()\n            outputs = kan_model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n        if epoch % 10 == 0:\n            print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")\n\ndef evaluate_kan_model(kan_model, data_loader, criterion):\n    kan_model.eval()\n    with torch.no_grad():\n        total_loss = 0\n        for inputs, targets in data_loader:\n            inputs, targets = inputs.float(), targets.float()\n            outputs = kan_model(inputs)\n            loss = criterion(outputs, targets)\n            total_loss += loss.item()\n    return total_loss / len(data_loader)\n","block_group":"ec357657427944a59c001651189dfda8","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"71e057d2b89848ed9bd5a70485045a7f","deepnote_cell_type":"markdown"},"source":"### 9.2: Prepare Data for KAN","block_group":"3a27c0946f884d508de46cae65ac2497"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"10b3412b793c47bda776af980e74a150","deepnote_cell_type":"code"},"source":"from torch.utils.data import TensorDataset, DataLoader\n\ndef prepare_kan_data(data_frame, target_column, batch_size=32):\n    inputs = data_frame.drop(columns=[target_column]).values\n    targets = data_frame[target_column].values\n    tensor_inputs = torch.tensor(inputs)\n    tensor_targets = torch.tensor(targets)\n    dataset = TensorDataset(tensor_inputs, tensor_targets)\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    return data_loader\n","block_group":"b300efa912a946c0a9013ce98ec45f20","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"cbc41cdab04447eab291b346f560a5ec","deepnote_cell_type":"markdown"},"source":"### 9.3: Train and Evaluate KAN Model","block_group":"1de806e3ac6f4359ba9173d7088e4bb2"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"13da9159a2994c48a8764032370582a9","deepnote_cell_type":"code"},"source":"# Prepare data for KAN model\ntarget_column = 'Evaluation Score'\nkan_data_loader = prepare_kan_data(combined_derandomized_dqn_data, target_column)\n\n# Initialize and train KAN model\ninput_size = combined_derandomized_dqn_data.shape[1] - 1  # Exclude target column\nhidden_size = 64\noutput_size = 1\n\nkan_model = KANModel(input_size, hidden_size, output_size)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(kan_model.parameters(), lr=0.001)\n\n# Train KAN model\ntrain_kan_model(kan_model, kan_data_loader, criterion, optimizer, num_epochs=100)\n\n# Evaluate KAN model\nkan_evaluation_loss = evaluate_kan_model(kan_model, kan_data_loader, criterion)\nprint(f\"KAN Model Evaluation Loss: {kan_evaluation_loss:.4f}\")\n","block_group":"e1cd7153de064ba0b13302b64dbf2024","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"24c94f4d94cc475298f8bc5451c6c85d","deepnote_cell_type":"markdown"},"source":"## Step 10: Visualize KAN Model and Results\n### 10.1: Visualize KAN Model Weights and Biases","block_group":"450628bea2834de59728fde89eb21ba1"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"a2c45269204b46feb9ed3c8a79b43e4a","deepnote_cell_type":"code"},"source":"def visualize_kan_weights_biases(kan_model):\n    for name, param in kan_model.named_parameters():\n        if param.requires_grad:\n            print(f\"{name}: {param.data}\")\n\nvisualize_kan_weights_biases(kan_model)\n","block_group":"084976d76b1441c79f3c00aeba529905","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"515f65c32d7f4e9cac9923ff1a254b51","deepnote_cell_type":"markdown"},"source":"### 10.2: Visualize the KAN Model Training Process","block_group":"ab7665dc235e47548abe9d3c1bf79824"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"faaf43682297445097d52e2af7f55282","deepnote_cell_type":"code"},"source":"import matplotlib.pyplot as plt\n\ndef plot_kan_training_loss(losses):\n    plt.figure(figsize=(10, 5))\n    plt.plot(losses, label='Training Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('KAN Model Training Loss')\n    plt.legend()\n    plt.show()\n\n# Assuming `losses` is a list of loss values collected during training\n# plot_kan_training_loss(losses)\n","block_group":"b25ffd8a24434883959053ec80a9add2","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"b3e49425dbff4590b2b974cbc134a18d","deepnote_cell_type":"markdown"},"source":"## Step 11: Extract and Interpret the Equivalence Formula\n### 11.1: Extract Symbolic Formula","block_group":"fcd86c0a69354a8388205ada38c13427"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"5b983ae9037848a48b512034f6e7b833","deepnote_cell_type":"code"},"source":"def extract_equivalence_formula(kan_model):\n    weights = kan_model.fc1.weight.data.numpy()\n    biases = kan_model.fc1.bias.data.numpy()\n    formula = \" + \".join([f\"{weights[0][i]}*x{i}\" for i in range(weights.shape[1])])\n    formula += f\" + {biases[0]}\"\n    return formula\n\nequivalence_formula = extract_equivalence_formula(kan_model)\nprint(f\"Equivalence Formula: {equivalence_formula}\")\n","block_group":"84a2fe9bf3624517a46e2b0542d00ed3","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"00442d5d903d48a0821b6c3b25b3caa2","deepnote_cell_type":"markdown"},"source":"### 11.2 Generate and Plot Equivalence Curves","block_group":"474491a65acb41bb84d48669e4283ed0"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"aa276444f81d467d8482a60b292c4b46","deepnote_cell_type":"code"},"source":"def generate_equivalence_curve(kan_model, data_loader):\n    equivalence_scores = []\n    for inputs, _ in data_loader:\n        inputs = inputs.float()\n        outputs = kan_model(inputs).detach().numpy()\n        equivalence_scores.extend(outputs)\n    return equivalence_scores\n\nequivalence_scores = generate_equivalence_curve(kan_model, kan_data_loader)\n\ndef plot_equivalence_curve(equivalence_scores):\n    plt.figure(figsize=(10, 5))\n    plt.plot(equivalence_scores, label='Equivalence Scores')\n    plt.xlabel('Samples')\n    plt.ylabel('Equivalence Score')\n    plt.title('Equivalence Curve')\n    plt.legend()\n    plt.show()\n\nplot_equivalence_curve(equivalence_scores)\n","block_group":"c15c1cacc4bf4b61afd251b01059dae5","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"83d7d91f1aef4afdad416b68f615610a","deepnote_cell_type":"markdown"},"source":"## 12. Summarize Results and Insights","block_group":"0f28471cfbea4aee9ac5250d403038ac"},{"cell_type":"code","metadata":{"source_hash":null,"deepnote_to_be_reexecuted":true,"cell_id":"0a3c7bf8d3b343da945feb79747a19ad","deepnote_cell_type":"code"},"source":"def summarize_results(equivalence_score_dqn, kan_evaluation_loss, equivalence_formula):\n    print(f\"Equivalence Score for DQN: {equivalence_score_dqn}\")\n    print(f\"KAN Model Evaluation Loss: {kan_evaluation_loss}\")\n    print(f\"Extracted Equivalence Formula: {equivalence_formula}\")\n    print(\"The equivalence score and KAN model insights suggest that the derandomized and probabilistic versions of the DQN-based Abel are closely related.\")\n    print(\"The extracted formula provides a mathematical representation of this relationship, further validating the potential equivalence between deterministic and probabilistic AI models in the context of chess.\")\n\nsummarize_results(equivalence_score_dqn, kan_evaluation_loss, equivalence_formula)\n","block_group":"3a488de415404e1f9cf0480a2718be11","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f6f51e1a-d40a-494a-8398-36807e7a81cb' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"95117a65edac4862b5b8af1e9da8b258","deepnote_execution_queue":[]}}